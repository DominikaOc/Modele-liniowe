---
title: "Modele liniowe Lista 3"
author: "Dominika Ochalik"
date: "2023-12-06"
output: 
  pdf_document:
    toc: yes
    toc_depth: 2
toc-title: "SPIS TREŚCI"
---

# Wstęp

Ten raport dotyczy analizy, jak dobrze dopasowana do danych jest prosta regresji. Analiza skupia się wokół obliczania współczynnika determinacji i korelacji, testu F, analizy wykresów, badania zachowania residuów oraz prób ulepszania modelu tak, aby uzyskać liniową zależność między danymi.

# Podstawowe elementy teorii

## Współczynnik R^2

Współczynnik determinacji $R^2$ określa, jaka część zmienności danych jest wyjaśniona przez model. Im większa wartość współczynnika, tym prosta regresji jest lepiej dopasowana do danych.

$R^2$ wyraża się wzorem:
$$R^2 = \frac{SSM}{SST}$$
gdzie:

- SSM - zmienność wyjaśniona przez model opisana wzorem: 
$$SSM = \sum_{i=1}^{n} (\hat Y_{i} - \bar Y)^2$$

- SST - zmienność całkowita opisana wzorem:
$$SST = \sum_{i=1}^{n} (Y_{i} - \bar Y)^2$$

```{r echo=F, warning=FALSE, message=FALSE}
library(readr)
tabela1_6 <- read_table("C:/Users/dario/Downloads/tabela1_6.txt",col_names = FALSE)
dane1 = tabela1_6
dane1 = as.data.frame(dane1)
colnames(dane1) = c("x1", "GPA", "IQ", "Płeć", "PH")
```

## Test F

Test F sprawdza, czy istnieje liniowa zależność między zmiennymi X i Y.
Problem sprowadza się do sprawdzenia, czy $b_{1}$=0.
$$H_{0}: b_{1}=0$$
Statystyka testowa F jest postaci: $$F = \frac{MSM}{MSE}$$
Odrzucamy $H_{0}$, gdy F>$F_{c}$, gdzie $F_{c}$ jest kwantylem rzędu $1-\alpha$ z rozkładu Fishera-Snedecora z dfM = 1 i dfE = n-2 stopniami swobody.

## Transformacja Box'a - Cox'a.

Jest to procedura używana w sytuacjach, gdy nie mamy liniowej zależności pomiędzy zmiennymi X i Y. Chcemy nałożyć pewną funkcję na Y tak, aby otrzymać liniową zależność. 

Otrzymujemy model postaci:
$$\tilde Y = b_{0} + b_{1}X + \epsilon$$
gdzie $\tilde Y = Y^{\lambda}$ lub $\tilde Y = \frac{Y^{\lambda}-1}{\lambda}$, gdy $\lambda \approx 0$.

*Uwaga: granicznym przekształceniem wyrażenia $\tilde Y = \frac{Y^{\lambda}-1}{\lambda}$ jest log(Y).*

Wartość parametru $\lambda$ estymujemy metodą największej wiarogodności.

\newpage
# Zadanie 1: Badanie zależności GPA od IQ.

## a) Równanie prostej regresji i współczynnik $R^2$.

W tym podpunkcie chcemy opisać zależność międzi GPA oraz wynikiem testu IQ. W tym celu znajdziemy równanie prostej regresji.

```{r echo=F}
model = lm(GPA ~ IQ, data = dane1)
x= dane1[,3]
y = dane1[,2]
```

Równanie prostej regresji jest postaci:

$$Y = b_{0} + b_{1}X$$

```{r echo=F}
b1 = model$coefficients[[2]]
b1 = round(b1, 4)
b0 = model$coefficients[[1]]
b0 = round(b0, 4)
```
gdzie:

- X to wartość wyniku testu IQ,

- Y to wartość GPA,

- $b_{0}$ wynosi `r b0`,
 
- $b_{1}$ wynosi `r b1`.

Zobaczmy dane i prostą regresji na wykresie:

```{r echo=F}
plot(x, y, col="blue", xlab = "IQ", ylab = "GPA", 
     main = "Wykres zależności między GPA i IQ")
abline(a = b0, b = b1, col="red")
```

Następnie obliczymy wartość współczynnika determinacji $R^2$ za pomocą wzorów teoretycznych i poleceń wbudowanych w R.

```{r echo=F, warning=F}
SST = sum((y - mean(y))^2)
y_hat = b0 + b1*x
SSM = sum((y_hat - mean(y))^2)
R_2 = SSM/SST
R_2 = round(R_2, 4)
```

Wartość obliczona za pomocą wzorów teoretycznych wynosi `r R_2`, natomiast wartość obliczona korzystając z funkcji wbudowanych w R wynosi `r round(summary(model)$r.squared, 4)`.

Na podstawie współczynnika $R^2$ możemy wywnioskować, jak dobrym przybliżeniem wartości i zachowania danych jest prosta regresji. Im większa wartość tego współczynnika, tym dopasowanie jest lepsze. W tym przypadku wartość $R^2$ wynosi `r R_2`. Biorąc pod uwagę analizę wykresu zależności między GPA i IQ możemy wywnioskować, że dane są dosyć szeroko oddalone od prostej regresji (mają dużą wariancję), co wpływa na to, że prosta regresji nie jest ich dobrym przybliżeniem.

## b) Test F.

Skoro ustalono, że prosta regresji nie jest dobrym przybliżeniem zależności między GPA i IQ, możemy przetestować hipotezę mówiącą, że GPA nie jest skorelowane z IQ. Zrobimy to za pomocą testu F.

Testujemy hipotezę zerową: $$H_{0}: b_{1}=0$$

```{r echo=F, warning=F}
SSM = sum((y_hat - mean(y))^2)
MSM = round(SSM, 4)
SSE = sum((y - y_hat)^2)
MSE = SSE/(length(x)-2)
MSE = round(MSE, 4)
F = MSM/MSE
F = round(F, 4)
```

W tym celu najpierw liczymy wartość statystyki testowej F:
$$F = \frac{MSM}{MSE}$$

Przyjmuje ona wartość F = `r round(F, 4)`.
Kwantyl $F^*(1-\alpha, 1, n-2)$ z rozkładu Fishera-Snedecora wynosi w przybliżeniu `r round(qf(0.95, 1, 76))`.
Widzimy, że F > $F^*$, zatem odrzucamy hipotezę zerową.
Ten sam wniosek możemy uzyskać, analizując p-wartość, czyli prawdopodobieństwo uzyskania takich wartości, jak F, i tych jeszcze mniej prawdopodobnych.

p-wartość możemy obliczyć za pomocą wyrażenia: 1- P( z>F ), gdzie z jest zmienną losową z rozkładu Fishera-Snedecora z 1 i n-2 stopniami swobody.

Wynosi ona: `r round(1-pf(F, 1, 76), 10)`.

Wartość statystyki F, jak i p-wartość, możemy obliczyć, korzystająć z poleceń wbudowanych w R.

Otrzymujemy:

```{r echo=F, warning=F}
F = anova(model)$`F value`[1]
F = round(F, 4)
p = anova(model)$`Pr(>F)`[1]
p = round(p, 10)
```

- F = `r F`,

- p-wartość = `r p`.

Widzimy, że p-wartość jest bardzo mała, a w szczególności mniejsza od $\alpha$=0.05, co pozwala nam odrzucić $H_{0}$.

## c) Przedziały predykcyjne dla IQ $\in$ {75, 100, 140}.

W tym podpunkcie należy przewidzieć wartość GPA dla uczniów, których IQ wynosi 75, 100, 140. W tym celu trzeba podać 90% przedziały predykcyjne.

```{r echo=F, warning=F}
zad1c = dane1[dane1$IQ == 100 | dane1$IQ == 75 | dane1$IQ == 140,]
s2 = sum((y - b0 - b1*x)^2)/(length(x)-2)
k = c(75, 100, 140)
lewy = numeric(3)
prawy = numeric(3)

s2_pred = function(k){
  return(s2*(1+ 1/nrow(dane1) + (k-mean(x))^2/sum((x-mean(x))^2)))
}

lewy = b0 + b1*k - sqrt(s2_pred(k))*qt(0.95, length(x)-2)
lewy = round(lewy, 4)
prawy = b0 + b1*k + sqrt(s2_pred(k))*qt(0.95, length(x)-2)
prawy = round(prawy, 4)
```

IQ | wartość oczekiwana GPA | lewy kraniec przedziału | prawy kraniec przedziału | Długość przedziału
----|---------------|------------|-----------------|--------
75  | `r b0+b1*75`  | `r lewy[1]`| `r prawy[1]` | `r prawy[1]-lewy[1]`
100 | `r b0+b1*100` | `r lewy[2]`| `r prawy[2]` | `r prawy[2]-lewy[2]`
140 | `r b0+b1*140` |`r lewy[3]` | `r prawy[3]` | `r prawy[3]-lewy[3]`

## d) Przedziały predykcyjne dla wszystkich obserwacji.

W tym podpunkcie chcemy dodać do wykresu z danymi 90% przedziały predykycjne.

```{r echo=F, warning=F}
lewy = numeric(length(x))
prawy = numeric(length(x))

lewy = b0 + b1*x - sqrt(s2_pred(x))*qt(0.95, length(x)-2)
lewy = round(lewy, 4)
prawy = b0 + b1*x + sqrt(s2_pred(x))*qt(0.95, length(x)-2)
prawy = round(prawy, 4)

library(ggplot2)
ggplot(dane1, aes(IQ, GPA))+geom_point()+
  annotate("segment", x=x, xend=x, y=lewy, yend=prawy, colour="blue",
           arrow = arrow(ends = "both", angle = 50, length = unit(.2,"cm")))
```
```{r echo=F}
nie_nalezy = numeric(length(x))
for(i in 1:length(x)){
  if(y[i]>=lewy[i] & y[i]<=prawy[i]) nie_nalezy[i]=0
  else nie_nalezy[i]=1
}
```

Widzimy, że tylko `r sum(nie_nalezy)` obserwacji nie należy do przedziałów predykcyjnych. Istnienie takich obserwacji wynika z faktu, że przedziały predykcyjne są konsruowane na poziomie ufności 90%, zatem spodziewamy się, że ok. 90% obserwacji wpadnie do przedziałów. W naszym przypadku  `r round(sum(nie_nalezy)/length(nie_nalezy), 4)`% obserwacji nie należy do przedziałów predykcyjnych, czyli ponad 90% obserwacji należy.

\newpage
# Zadanie 2: Badanie zależności GPA od PH.

## a) Prosta regresji i współczynnik $R^2$.

W tym zadaniu będziemy badać zależność między GPA oraz wyników testu HP.
W celu znalezienia równania prostej regresji, najpierw znajdziemy wartości współczynników $b_{0}$ i $b_{1}$.

```{r echo=F, warning=F}
model = lm(GPA ~ PH, data = dane1)
x= dane1[,5]
y = dane1[,2]

b1 = model$coefficients[[2]]
b1 = round(b1, 4)
b0 = model$coefficients[[1]]
b0 = round(b0, 4)
```

Wynoszą one:

- $b_{0}$ = `r b0`,

- $b_{1}$ = `r b1`.

Zobaczmy dane na wykresie wraz z dorysowaną prostą regresji:

```{r echo=F, warning=F}
plot(x, y, col="orange", xlab = "PH", ylab = "GPA", 
     main = "Wykres zależności między GPA i PH")
abline(a = b0, b = b1, col="blue")
```

Następnie obliczymy wartość współczynnika determinacji $R^2$ za pomocą wzorów teoretycznych i poleceń wbudowanych w R.

```{r echo=F, warning=F}
SST = sum((y - mean(y))^2)
y_hat = b0 + b1*x
SSM = sum((y_hat - mean(y))^2)
R_2 = SSM/SST
R_2 = round(R_2, 4)
```

Wartość obliczona za pomocą wzorów teoretycznych wynosi `r R_2`, natomiast wartość obliczona korzystając z funkcji wbudowanych w R wynosi `r round(summary(model)$r.squared, 4)`. Widzimy, że jest ona jeszcze mniejsza niż w przypadku analizy zależności między GPA i IQ. Ponadto, z wykresu możemy wywnioskować, że punkty są szeroko oddalone od prostej regresji (mają dużą wariancję).

## b) Test F.

Skoro ustalono, że prosta regresji nie jest dobrym przybliżeniem zależności między GPA i PH, możemy przetestować hipotezę mówiącą, że GPA nie jest skorelowane z PH. Zrobimy to za pomocą testu F.

Testujemy hipotezę zerową: $$H_{0}: b_{1}=0$$

```{r echo=F, warning=F}
SSM = sum((y_hat - mean(y))^2)
MSM = round(SSM, 4)
SSE = sum((y - y_hat)^2)
MSE = SSE/(length(x)-2)
MSE = round(MSE, 4)
F = MSM/MSE
```

W tym celu najpierw liczymy wartość statystyki testowej F:
$F = \frac{MSM}{MSE}$

Przyjmuje ona wartość F = `r round(F, 4)`.
Kwantyl $F^*(1-\alpha, 1, n-2)$ z rozkładu Fishera-Snedecora wynosi w przybliżeniu `r round(qf(0.95, 1, 76))`.
Widzimy, że F > $F^*$, zatem odrzucamy hipotezę zerową.

Ten sam wniosek możemy uzyskać, analizując p-wartość, czyli prawdopodobieństwo uzyskania takich wartości, jak F, i tych jeszcze mniej prawdopodobnych.

p-wartość możemy obliczyć za pomocą wyrażenia: 1- P( z>F ), gdzie z jest zmienną losową z rozkładu Fishera-Snedecora z 1 i n-2 stopniami swobody.

Wynosi ona: `r round(1-pf(F, 1, 76), 10)`.

Wartość statystyki F, jak i p-wartość, możemy obliczyć, korzystająć z poleceń wbudowanych w R.

Otrzymujemy:

```{r echo=F, warning=F}
F = anova(model)$`F value`[1]
F = round(F, 4)
p = anova(model)$`Pr(>F)`[1]
p = round(p, 10)
```

- F = `r F`,

- p-wartość = `r p`.

Widzimy, że p-wartość jest bardzo mała, a w szczególności mniejsza od $\alpha$=0.05, co pozwala nam odrzucić $H_{0}$.

## c) Przedziały predykcyjne dla PH $\in$ {25, 55, 85}.

W tym zadaniu chcemy przewidzieć wynik GPA dla uczniów, których wyniki testu PH wynoszą 25, 55 lub 85. W tym celu skonstruujemy 90% przedziały predykcyjne.

```{r echo=F}
s2 = sum((y - b0 - b1*x)^2)/(length(x)-2)
k = c(25, 55, 85)
lewy = numeric(3)
prawy = numeric(3)

s2_pred = function(k){
  return(s2*(1+ 1/nrow(dane1) + (k-mean(x))^2/sum((x-mean(x))^2)))
}

lewy = b0 + b1*k - sqrt(s2_pred(k))*qt(0.95, length(x)-2)
lewy = round(lewy, 4)
prawy = b0 + b1*k + sqrt(s2_pred(k))*qt(0.95, length(x)-2)
prawy = round(prawy, 4)
```

PH | wartość oczekiwana GPA | lewy kraniec przedziału | prawy kraniec przedziału | Długość przedziału
---|--------------|------------|------------------|-------
25 | `r b0+b1*25` | `r lewy[1]`| `r prawy[1]` | `r prawy[1]-lewy[1]`
55 | `r b0+b1*55` | `r lewy[2]`| `r prawy[2]` | `r prawy[2]-lewy[2]`
85 | `r b0+b1*85` |`r lewy[3]` | `r prawy[3]` | `r prawy[3]-lewy[3]`

## d) Przedziały predykcyjne dla wszystkich obserwacji.

W tym podpunkcie dodamy do wykresu z danymi 90% przedziały predykcyjne, a następnie ustalimy, ile obserwacji znajduje się poza granicami przedziałów.

```{r echo=F, warning=F}
lewy = numeric(length(x))
prawy = numeric(length(x))

lewy = b0 + b1*x - sqrt(s2_pred(x))*qt(0.95, length(x)-2)
lewy = round(lewy, 4)
prawy = b0 + b1*x + sqrt(s2_pred(x))*qt(0.95, length(x)-2)
prawy = round(prawy, 4)

library(ggplot2)
ggplot(dane1, aes(PH, GPA))+geom_point()+
  annotate("segment", x=x, xend=x, y=lewy, yend=prawy, colour="orange",
           arrow = arrow(ends = "both", angle = 50, length = unit(.2,"cm")))
```

```{r echo=F}
nie_nalezy = numeric(length(x))
for(i in 1:length(x)){
  if(y[i]>=lewy[i] & y[i]<=prawy[i]) nie_nalezy[i]=0
  else nie_nalezy[i]=1
}
```

Widzimy, że tylko `r sum(nie_nalezy)` obserwacji nie należy do przedziałów predykcyjnych. Istnienie takich obserwacji wynika z faktu, że przedziały predykcyjne są konsruowane na poziomie ufności 90%, zatem spodziewamy się, że ok. 90% obserwacji wpadnie do przedziałów. W naszym przypadku  `r round(sum(nie_nalezy)/length(nie_nalezy), 4)`% obserwacji nie należy do przedziałów predykcyjnych, czyli ponad 90% obserwacji należy.

```{r echo=F, warning=F, message=F}
library(readr)
CH01PR20 <- read_table("~/modele liniowe/CH01PR20.txt", 
    col_names = FALSE)
dane = CH01PR20
dane=as.data.frame(dane)
colnames(dane) = c("Czas", "Kopiarki")

model=lm(Czas ~ Kopiarki, data=dane)
x = dane[,2] # Kopiarki
y = dane[,1] # Czas
```

## Która ze zmiennych: IQ czy PH jest lepszy predyktorem GPA?

Podsumowując wszystkie uzyskane do tej pory wnioski możemy stwierdzić, że wynik testu IQ jest lepszym predyktorem wartości GPA. Postawiona teza wynika z faktu, że dla IQ i GPA mamy większą wartość współczynnika $R^2$ oraz przedziały predykcyjne mają trochę mniejszą długość.

\newpage
# Zadanie 3

W zadaniu 3 korzystamy z danych z pliku ch01pr20.txt, który zawiera informacje na temat liczby kopiarek (druga kolumna) i czasu potrzebnego na ich utrzymanie (pierwsza kolumna).

## a) Sprawdzenie, czy suma residuów jest równa 0.

```{r echo=F}
b1 = round(model$coefficients[[2]], 4)
b0 = round(model$coefficients[[1]], 4)

y_hat = b0 + b1*x
errors = y_hat - y
suma = round(sum(errors), 4)
```

Najpierw należy znaleźć równanie regresji liniowej. W tym celu potrzebne nam będą wartości estymatorów $b_{0}$ i $b_{1}$, które wynoszą odpowiednio:

- $b_{0}$ = `r round(b0, 4)`

- $b_{1}$ = `r round(b1, 4)`.

W równaniu regresji występują też zmienne X i Y, które w tym przypadku oznaczają odpowiednio liczbę kopiarek i czas.

Estymator Y, czyli $\hat Y$ liczymy ze wzoru $$\hat Y =  b_{0} + b_{1} \cdot X$$ dla obliczonych wyżej wartości $b_{0}$ i $b_{1}$.

Suma residuów to suma błędów, czyli wyrażenie postaci: $$ \sum_{i=1}^{n}(\hat Y_{i} - Y_{i})$$
które wynosi `r suma`. Widzimy, że jest różne od zera.

## b) Wykres residuów względem zmiennej X.

W tym podpunkcie należy przedstawić wykres residuów względem zmiennej objaśniającej.

```{r echo=F}
plot(x, errors, ylab="Wartość residuum", xlab="Liczba kopiarek",
     main = "Wykres residuum względem zmiennej objaśniającej (Kopiarki)",
     pch=16, col="pink")
```

Większość wartości residuów mieści się w przedziale od -10 do 10 i są losowo rozrzucone w tym przedziale. Nie widać wyraźnej zależności pomiędzy kolejnymi punktami. Możemy zauważyć dwie obserwacje odstające, jedną dla X=8, a drugą dla X=10.

## c) Wykres residuów względem kolejności występowania.

```{r echo=F}
plot(1:length(errors), errors, ylab = "Wartość residuów", 
     xlab = "Kolejność występowania w pliku danych", 
     main = "Wykres residuów względem kolejności występowania w pliku",
     pch=16, col="orange")
```
Nie widać zależności między kolejnymi wartościami błędów; wszystkie punkty zdają się być losowo rozrzucone między wartościami -10 i 10 (w większości przypadków). Możemy zauważyć, że są dwie obserwacje odstające, które osiagają wartość residuum blisko 20, a w pliku występują między 15 a 20 pozycją.

## d) Badanie normalności rozkładu residuów.
 
W tym podpunkcie będziemy badać normalność rozkładu residuów za pomocą histogramu oraz wykresu kwantylowo-kwantylowego.

```{r echo=F}
hist(errors, col="pink", main = "Histogram residuów",
     xlab = "Wartość residuum", ylab = "Częstotliwość występowania")

qqnorm(errors, col="blue", main = "Wykres kwantylowo-kwantylowy dla residuów", 
       xlab = "Kwantyle teoretyczne", ylab = "Kwantyle próbkowe")
```

Analizując histogram oraz wykres kwantylowo-kwantylowy możemy wywnioskować, że rozkład residuów jest w przybliżeniu normalny.

\newpage
# Zadanie 4: Dane z dodaną obserwacją (1000; 2).

```{r echo=F}
dane2 = dane
dane2[46,] = c(1000, 2)
x2 = dane2[,2]
y2 = dane2[,1]
model2 = lm(Czas~Kopiarki, data=dane2)
b02 = round(model2$coefficients[[1]], 4)
b12 = round(model2$coefficients[[2]], 4)
p2 = anova(model2)$`Pr(>F)`[1]
p2 = round(p2, 4)
p = anova(model)$`Pr(>F)`[1]
p = round(p, 4)
t = summary(model)$coefficients[6]
t = round(t, 4)
t2 = summary(model2)$coefficients[6]
t2 = round(t2, 4)
errors2 = b02 + b12*x2 - y2
```

## a) Tabela porównująca.

Parametry | Oryginalne dane | Dane z dodaną wartością
----------|-----------------|------------------------
Równanie regresji |  Y = `r b0` + `r b1`X | Y = `r b02` + `r b12`X
Wartość statystyki t | `r t` | `r t2`
p-wartość | `r p` | `r p2`
$R^2$ | `r summary(model)$'r.squared'` | `r summary(model2)$'r.squared'`
Estymator wariancji | `r sd(errors)^2` | `r sd(errors2)^2`

Widzimy, że dla danych z dodaną wartością współczynnik $R^2$ osiąga bardzo małą wartość, co oznacza, że prosta regresji nie przybliża dobrze wartości i zachowania danych.

## b) 

Teraz będziemy powtarzać podpunkty b), c) i d) z zadania 3 na zmodyfikowanym w tym zadaniu zbiorze danych.

```{r echo=F}
plot(x2, errors2, ylab="Wartość residuum", xlab="Liczba kopiarek",
     main = "Wykres residuum względem zmiennej objaśniającej (kopiarek)",
     type="p", pch=16, col="violet")
```

Widzimy, że dodana obserwacja bardzo wyróżnia się na powyższym wykresie ze względu na wartość odstającą zmiennej X (czasu). Implikuje to wyraźnie większą wartość residuum. Pozostałe obserwacje osiągają podobną wartość residuum.

```{r echo=F}
plot(1:length(errors2), errors2, ylab = "Wartość residuów", 
     xlab = "Kolejność występowania w pliku danych", 
     main = "Wykres residuów względem kolejności występowania w pliku danych", type="p", pch=16, col="lightblue")
```

Ponownie możemy dostrzec odstającą wartość residuum dla dodanej zmiennej. Co ciekawe, wartości błędów pozostałych obserwacji zdają się być od siebie zależne, ponieważ nie są rozmieszczone losowo, tylko układają się w kształt fali (językiem matematyka: w kształt sinusoidy).

Zbadajmy normalność wartości residuów:

```{r echo=F}
hist(errors2, col="pink", main = "Histogram residuów",
     xlab = "Wartość residuum", ylab = "Częstotliwość występowania")

qqnorm(errors2, col="blue", main = "Wykres kwantylowo-kwantylowy dla residuów", 
       xlab = "Kwantyle teoretyczne", ylab = "Kwantyle próbkowe")
```

Widzimy, że histogram nie przypomina swoim kształtem krzywej Gaussa. Należy też zauważyć, że ze względu na obserwację odstającą nie da się wiele z niego odczytać.

Punkty na wykresie kwantylowo-kwantylowym ze względu na obserwację odstającą sprawiają wrażenie, jakby bardziej układały się w linię prostą. Obserwacja odstająca jest bardzo charakterystyczna na wykresie (podobnie, jak na histogramie).

## c) Dodanie obserwacji (1000; 6) do początkowego pliku.

```{r echo=F}
dane3 = dane
dane3[46,] = c(1000, 6)
x3 = dane3[,2]
y3 = dane3[,1]
model3 = lm(Czas~Kopiarki, data=dane3)
b03 = round(model3$coefficients[[1]], 4)
b13 = round(model3$coefficients[[2]], 4)
p3 = anova(model3)$`Pr(>F)`[1]
p3 = round(p2, 4)
t3 = summary(model3)$coefficients[6]
t3 = round(t3, 4)
errors3 = b03 + b13*x3 - y3
```

Parametry | Oryginalne dane | Dane z dodaną wartością
----------|-----------------|------------------------
Równanie regresji |  Y = `r b0` + `r b1`X | Y = `r b03` + `r b13`X
Wartość statystyki t | `r t` | `r t3`
p-wartość | `r p` | `r p3`
$R^2$ | `r summary(model)$'r.squared'` | `r summary(model3)$'r.squared'`
Estymator wariancji | `r sd(errors)^2` | `r sd(errors3)^2`

```{r echo=F}
plot(x3, errors3, ylab="Wartość residuum", xlab="Liczba kopiarek",
     main = "Wykres residuum względem zmiennej objaśniającej (Kopiarek)",
     pch=16, col=15)
```

Dodana obserwacja wyróżnia się na wykresie ze względu na bardzo dużą wartość residuum. Pozostałe obserwacje mają zbliżone do siebie wartości residuów.

```{r echo=F}
plot(1:length(errors3), errors3, ylab = "Wartość residuów", 
     xlab = "Kolejność występowania w pliku danych", 
     main = "Wykres residuów względem kolejności występowania w pliku",
     pch=16, col=15)
```

Dodana obserwacja znowu wyróżnia się na wykresie. Pozostałe punkty na wykresie mają zbliżone do siebie wartości residuów.

Zbadajmy normalność wartości resiuów:

```{r echo=F}
hist(errors3, col="pink", main = "Histogram residuów",
     xlab = "Wartość residuum", ylab = "Częstotliwość występowania")

qqnorm(errors3, col="blue", main = "Wykres kwantylowo-kwantylowy dla residuów", 
       xlab = "Kwantyle teoretyczne", ylab = "Kwantyle próbkowe", pch=16)
```

Histogram, ze względu na wartość odstającą, jest bardzo nieczytelny i niewiele można z niego odczytać. Na wykresie kwantylowo-kwantylowym widzimy, że wszystkie obserwacje (poza jedną odstającą) układają się w linię prostą.

\newpage
# Zadanie 5

W tym zadaniu będziemy pracować z danymi z pliku ch03pr15.txt. W pliku znajdują się dane na temat stężenia roztworu (pierwsza kolumna) i czasu (druga kolumna).

```{r echo=F, warning=F, message=F}
library(readr)
CH03PR15 <- read_table("CH03PR15.txt", col_names = FALSE)
dane3 = CH03PR15
dane3 = as.data.frame(dane3)
colnames(dane3) = c("Stężenie", "Czas")
model3 = lm(Stężenie ~ Czas, data=dane3)
x3 = dane3[,2]
y3 = dane3[,1]
```

## a) Równanie regresji i 95% przedziały predykcyjne.

Aby znaleźć równanie regresji, potrzebujemy znaleźć wartości współczynników $b_{0}$ i $b_{1}$. Ponadto, stężenie roztworu będzie zmienną odpowiedzi (Y w równaniu), a czas zmienną objaśniającą (X).

Wartości współczynników $b_{0}$ i $b_{1}$ obliczone przy użyciu poleceń wbudowanych w R:

```{r echo=F}
b03 = model3$coefficients[[1]]
b03 = round(b03, 4)
b13 = model3$coefficients[[2]]
b13 = round(b13, 4)
```

- $b_{0}$ = `r b03`,

- $b_{1}$ = `r b13`.

Zatem równanie regresji wyraża się wzorem: Y = `r b03` + `r b13`X.

Zobaczmy dane wraz z prostą regresji na wykresie:

```{r echo=F}
plot(x3, y3, xlab = "Czas", ylab = "Wartość stężenia", 
     main = "Wykres zależności wartości stężenia od czasu", 
     col = "violet", pch=16)
abline(a = b03, b= b13, col="blue")
```

Dodajmy do wykresu 95% przedziały predykcyjne:

```{r echo=F}
s2 = sum((y3 - b03 - b13*x3)^2)/(length(x3)-2)

s2_pred = function(k){
  return(s2*(1+ 1/nrow(dane3) + (k-mean(x3))^2/sum((x3-mean(x3))^2)))
}

lewy = numeric(length(x3))
prawy = numeric(length(x3))

lewy = b03 + b13*x3 - sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
lewy = round(lewy, 4)
prawy = b03 + b13*x3 + sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
prawy = round(prawy, 4)

library(ggplot2)
ggplot(dane3, aes(Czas, Stężenie))+geom_point()+
  annotate("segment", x=x3, xend=x3, y=lewy, yend=prawy, colour="orange",
           arrow = arrow(ends = "both", angle = 50, length = unit(.2,"cm")))
```

Widzimy, że wszystkie obserwacje mieszczą się w wyznaczonych przedziałach predykcyjnych, które są dosyć szerokie.

## b) Analiza regresji: współczynnik $R^2$ i test istotności dla $b_{1}$.

W celu zbadania dokładności dopasowania prostej regresji do danych obliczymy współczynnik $R^2$. 

```{r echo=F}
R2 = summary(model3)$'r.squared'
R2 = round(R2, 4)
```

Wynosi on: `r R2`. Widzimy, że jest dosyć duży, co sugeruje, że prosta regresji jest dobrze dopasowana do danych.

Zbadajmy, czy istnieje zależność między zmiennymi. W tym celu testujemy nastepującą hipotezę:

$$H_{0}: b_{1}=0$$
Hipoteza alternatywna zaprzecza hipotezie zerowej, czyli jest postaci:
$$H_{a}: b_{1} \neq 0$$
W celu przetestowania hipotez możemy użyć testu t-studenta albo testu Fishera. Z racji, że są równoważne, nie ma znaczenia, który test wybierzemy. Zbadajmy zatem hipotezę zerową przy pomocy testu F.

Tradycyjnie przyjmujemy $\alpha$=0.05.

Statystyka testowa jest postaci:
$$F = \frac{MSM}{MSE}$$
```{r echo=F}
F = anova(model3)$`F value`[1]
F = round(F, 4)
```

Podstawiając, otrzymujemy: F = `r F`.

Teraz sprawdzimy, czy wartość statystyki F jest większa od kwantyla $F_{c} = F^*$(0.95, 1, `r length(y3)-2`), który wynosi `r round(qf(0.95, 1, length(y3)-2), 4)`. 

Widzimy, że F > $F_{c}$, zatem odrzucamy $H_{0}$ mówiącą, że $b_{1}$=0.

Ten sam wniosek możemy uzyskać analizując p-wartość, która wynosi `r 1-pf(F, 1, length(y3)-2)`. Widzimy, że jest zdecydowanie mniejsza niż $\alpha$=0.05, zatem odrzucamy $H_{0}$.

**Wniosek**: Z prawdopodobieństwem 95% wartość $b_{1} \neq 0$, co oznacza, że istnieje zależność między stężeniem roztworu a czasem.

## c) Współczynnik korelacji między przewidywaną a obserwowaną wartością roztworu.

```{r echo=F}
licznik = sum((x3 - mean(x3))*(y3 - mean(y3)))
mianownik2 = sum((x3 - mean(x3))^2)*sum((y3 - mean(y3))^2)
mianownik = sqrt(mianownik2)
r = licznik/mianownik
r = round(r, 4)
```

Współczynnik korelacji wynosi `r r`. Jest ujemny i bliski -1, co oznacza, że istnieje silny związek między stężeniem roztworu a czasem. Ujemna wartość współczynnika oznacza, że wraz ze wzrostem czasu wartość stężenia maleje.

# Zadanie 6: procedura Box'a-Cox'a.

```{r echo=F, warning=F}
library(MASS)
b = boxcox(model3)
lambda=b$x[which.max(b$y)]
lambda = round(lambda, 4)
```
Wartość parametru $\lambda$ wynosi `r lambda`. Widzimy, że jest prawie równa zero, co oznacza, że dokonujemy podstawienia:
$$\tilde Y = log(Y)$$

\newpage
# Zadanie 7

## Utworzenie nowej zmiennej odpowiedzi.

Wykorzystując procedurę Box'a - Cox'a, tworzymy nową zmienną odpowiedzi $\tilde Y$, która wyraża się wzorem $\tilde Y = log(Y)$.

```{r echo=F}
dane3[,3] = log(dane3[,1])
colnames(dane3) = c("Stężenie", "Czas", "newY")
```

## Powtórz zadanie 5 dla nowej zmiennej odpowiedzi.

Sprawdzimy, jak zmienią się uzyskane wnioski, gdy powtórzymy wszystkie kroki z zadania 5 dla nowej zmiennej odpowiedzi $\tilde Y$.

### Regresja liniowa i przedziały predykcyjne.

```{r echo=F}
model3 = lm(newY ~ Czas, data=dane3)
x3 = dane3[,2]
y3 = dane3[,3]
b03 = model3$coefficients[[1]]
b03 = round(b03, 4)
b13 = model3$coefficients[[2]]
b13 = round(b13, 4)
```

Wartości współczynników wynoszą:

- $b_{0}$ = `r b03`,

- $b_{1}$ = `r b13`.

Zatem równanie regresji wyraża się wzorem: Y = `r b03` + `r b13`X.

Zobaczmy dane wraz z prostą regresji na wykresie:

```{r echo=F}
plot(x3, y3, xlab = "Czas", ylab = "Logarytm wartości stężenia", 
     main = "Wykres zależności nowej zmiennej odpowiedzi od czasu", 
     col = "violet", pch=16)
abline(a = b03, b= b13, col="blue")
```
Patrząc na wykres widzimy, że dane bardziej układają się w linię prostą niż w poprzednim przypadku.

Dodajmy do wykresu 95% przedziały predykcyjne:

```{r echo=F}
s2 = sum((y3 - b03 - b13*x3)^2)/(length(x3)-2)

s2_pred = function(k){
  return(s2*(1+ 1/nrow(dane3) + (k-mean(x3))^2/sum((x3-mean(x3))^2)))
}

lewy = numeric(length(x3))
prawy = numeric(length(x3))

lewy = b03 + b13*x3 - sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
lewy = round(lewy, 4)
prawy = b03 + b13*x3 + sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
prawy = round(prawy, 4)

library(ggplot2)
ggplot(dane3, aes(Czas, newY))+geom_point()+
  annotate("segment", x=x3, xend=x3, y=lewy, yend=prawy, colour="orange",
           arrow = arrow(ends = "both", angle = 50, length = unit(.2,"cm")))
```

Widzimy, że wszystkie obserwcje mieszczą się w wyznaczonych przedziałąch predykcyjnych, które mają dość małą długość. Oznacza to, że dobrze przybliżają wartości zmiennych odpowiedzi.

### Analiza regresji: współczynnik $R^2$ i test istotności dla $b_{1}$.

W celu zbadania dokładności dopasowania prostej regresji do danych obliczymy współczynnik $R^2$. 

```{r echo=F}
R2 = summary(model3)$'r.squared'
R2 = round(R2, 4)
```

Wynosi on: `r R2`. Widzimy, że jest prawie równy 1, co oznacza, że prosta regresji jest bardzo dobrze dopasowana do danych.

Zbadajmy, czy istnieje zależność między zmiennymi. W tym celu testujemy nastepującą hipotezę:

$$H_{0}: b_{1}=0$$
Hipoteza alternatywna zaprzecza hipotezie zerowej, czyli jest postaci:
$$H_{a}: b_{1} \neq 0$$
W celu przetestowania hipotez możemy użyć testu t-studenta albo testu Fishera. Z racji, że są równoważne, nie ma znaczenia, który test wybierzemy. Zbadajmy zatem hipotezę zerową przy pomocy testu F.

Tradycyjnie przyjmujemy $\alpha$=0.05.

Statystyka testowa jest postaci:
$$F = \frac{MSM}{MSE}$$
```{r echo=F}
F = anova(model3)$`F value`[1]
F = round(F, 4)
```

Podstawiając, otrzymujemy: F = `r F`.

Teraz sprawdzimy, czy wartość statystyki F jest większa od kwantyla $F_{c} = F^*$(0.95, 1, `r length(y3)-2`), który wynosi `r round(qf(0.95, 1, length(y3)-2), 4)`. 

Widzimy, że F > $F_{c}$, zatem odrzucamy $H_{0}$ mówiącą, że $b_{1}$=0.

Ten sam wniosek możemy uzyskać analizując p-wartość, która wynosi `r 1-pf(F, 1, length(y3)-2)`. Widzimy, że jest zdecydowanie mniejsza niż $\alpha$=0.05, zatem odrzucamy $H_{0}$.

**Wniosek**: Z prawdopodobieństwem 95% wartość $b_{1} \neq 0$, co oznacza, że istnieje zależność między stężeniem roztworu a czasem.

### Współczynnik korelacji między przewidywaną a obserwowaną wartością roztworu.

```{r echo=F}
licznik = sum((x3 - mean(x3))*(y3 - mean(y3)))
mianownik2 = sum((x3 - mean(x3))^2)*sum((y3 - mean(y3))^2)
mianownik = sqrt(mianownik2)
r = licznik/mianownik
r = round(r, 4)
```

Współczynnik korelacji wynosi `r r`. Jest ujemny i bliski -1, co oznacza, że istnieje silny związek między stężeniem roztworu a czasem. Ujemna wartość współczynnika oznacza, że wraz ze wzrostem czasu wartość stężenia maleje.

Porównując uzyskaną wartość z tą uzyskaną w zadaniu 5, widzimy, że wartość współczynnika korelacji dla zmienionej zmiennej odpowiedzi jest co do modułu bliższa 1, co oznacza, że korelacja jest silniejsza.

Podsumowując, model ze zmienioną zmienną odpowiedzi $\tilde Y$ jest dużo lepszy od pierwotnego.

Przeanalizujmy jeszcze jeden model.

\newpage
# Zadanie 8: nowa zmienna objaśniająca.

W tym zadaniu przekształcamy zmienną objaśniającą X zgodnie ze wzorem:
$$\tilde X = X^{-0.5}$$

```{r echo=F}
dane3[,4] = dane3[,2]^(-0.5)
colnames(dane3) = c("Stężenie", "Czas", "newY", "newX")
```

Powtarzamy kroki z zadania 7 dla $\tilde X$ jako zmiennej objaśniającej oraz Y jako zmiennej odpowiedzi.

## Regresja liniowa i przedziały predykcyjne.

```{r echo=F}
model3 = lm(Stężenie ~ newX, data=dane3)
x3 = dane3[,4]
y3 = dane3[,1]
b03 = model3$coefficients[[1]]
b03 = round(b03, 4)
b13 = model3$coefficients[[2]]
b13 = round(b13, 4)
```

Wartości współczynników wynoszą:

- $b_{0}$ = `r b03`,

- $b_{1}$ = `r b13`.

Zatem równanie regresji wyraża się wzorem: Y = `r b03` + `r b13`X.

Zobaczmy dane wraz z prostą regresji na wykresie:

```{r echo=F}
plot(x3, y3, xlab = "Czas", ylab = "Stężenie", 
     main = "Wykres zależności nowej zmiennej objaśniającej od czasu", 
     col = "violet", pch=16)
abline(a = b03, b= b13, col="blue")
```
Widzimy, że prosta na wykresie zmieniła kierunek: jest funkcją liniową rosnącą. Jest mniej dopasowana do danych niż w przypadku poprzedniego modelu, ale lepiej niż w pierwotnym modelu.

Dodajmy do wykresu 95% przedziały predykcyjne:

```{r echo=F}
s2 = sum((y3 - b03 - b13*x3)^2)/(length(x3)-2)

s2_pred = function(k){
  return(s2*(1+ 1/nrow(dane3) + (k-mean(x3))^2/sum((x3-mean(x3))^2)))
}

lewy = numeric(length(x3))
prawy = numeric(length(x3))

lewy = b03 + b13*x3 - sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
lewy = round(lewy, 4)
prawy = b03 + b13*x3 + sqrt(s2_pred(x3))*qt(0.975, length(x3)-2)
prawy = round(prawy, 4)

library(ggplot2)
ggplot(dane3, aes(newX, Stężenie))+geom_point()+
  annotate("segment", x=x3, xend=x3, y=lewy, yend=prawy, colour="orange",
           arrow = arrow(ends = "both", angle = 50, length = unit(.2,"cm")))
```

Widzimy, że przedziały są nieco dłuższe niż w poprzednim modelu, ale węższe niż w pierwotnym. Wszystkie obserwacje wpadają do przedziałów predykcyjnych.

## Analiza regresji: współczynnik $R^2$ i test istotności dla $b_{1}$.

W celu zbadania dokładności dopasowania prostej regresji do danych obliczymy współczynnik $R^2$. 

```{r echo=F}
R2 = summary(model3)$'r.squared'
R2 = round(R2, 4)
```

Wynosi on: `r R2`. Widzimy, że jest prawie równy 1, co oznacza, że prosta regresji jest bardzo dobrze dopasowana do danych.

Zbadajmy, czy istnieje zależność między zmiennymi. W tym celu testujemy nastepującą hipotezę:

$$H_{0}: b_{1}=0$$
Hipoteza alternatywna zaprzecza hipotezie zerowej, czyli jest postaci:
$$H_{a}: b_{1} \neq 0$$
W celu przetestowania hipotez możemy użyć testu t-studenta albo testu Fishera. Z racji, że są równoważne, nie ma znaczenia, który test wybierzemy. Zbadajmy zatem hipotezę zerową przy pomocy testu F.

Tradycyjnie przyjmujemy $\alpha$=0.05.

Statystyka testowa jest postaci:
$$F = \frac{MSM}{MSE}$$
```{r echo=F}
F = anova(model3)$`F value`[1]
F = round(F, 4)
```

Podstawiając, otrzymujemy: F = `r F`.

Teraz sprawdzimy, czy wartość statystyki F jest większa od kwantyla $F_{c} = F^*$(0.95, 1, `r length(y3)-2`), który wynosi `r round(qf(0.95, 1, length(y3)-2), 4)`. 

Widzimy, że F > $F_{c}$, zatem odrzucamy $H_{0}$ mówiącą, że $b_{1}$=0.

Ten sam wniosek możemy uzyskać analizując p-wartość, która wynosi `r 1-pf(F, 1, length(y3)-2)`. Widzimy, że jest zdecydowanie mniejsza niż $\alpha$=0.05, zatem odrzucamy $H_{0}$.

**Wniosek**: Z prawdopodobieństwem 95% wartość $b_{1} \neq 0$, co oznacza, że istnieje zależność między stężeniem roztworu a czasem.

## Współczynnik korelacji między przewidywaną a obserwowaną wartością roztworu.

```{r echo=F}
licznik = sum((x3 - mean(x3))*(y3 - mean(y3)))
mianownik2 = sum((x3 - mean(x3))^2)*sum((y3 - mean(y3))^2)
mianownik = sqrt(mianownik2)
r = licznik/mianownik
r = round(r, 4)
```

Współczynnik korelacji wynosi `r r`. Jest dodatni i bliski 1, co oznacza, że istnieje silny związek między stężeniem roztworu a czasem. Dodatnia wartość współczynnika oznacza, że wraz ze wzrostem czasu wartość stężenia również wzrasta.

## Który model jest najlepszy?

Podsumowując, najlepszy jest model log(Y)~X. Trochę gorszy, aczkolwiek dalej dobry i lepszy od pierwitnego, okazał się model Y~$X^{-0.5}$. Wnioski zostały wyciągniete na podstawie:

- analizy wartoścy $R^2$ - dla modelu log(Y)~X jego wartość była największa, 

- długości przedziałów predykcyjnych - dla modelu log(Y)~X przedziały osiągały najmniejszą długość.

\newpage
# Zadania teoretyczne

## Zadanie 1

### Obliczanie wartości krytycznej t-testu

Za pomocą R należy obliczyć wartość krytyczną($t_c$) dla dwukierunkowego t-testu istotności z r stopniami swobody, gdzie:

- $r \in \{5, 10, 50\}$,

- $\alpha$ = 0.05.

liczba stopni swobody | wartość $t_c$
----------------------|---------------
5 | `r round(qt(0.975, 5), 4)`
10 | `r round(qt(0.975, 10), 4)`
50 | `r round(qt(0.975, 50), 4)`

### Wartość krytyczna testu F

Za pomocą R należy obliczyć wartość krytyczną ($F_c$) dla testu istotności F z 1 stopniem swobody w liczniku i r stopniami swobody w mianowniku, gdzie:

- $r \in \{5, 10, 50\}$,

- $\alpha$ = 0.05.

liczba stopni swobody w mianowniku | wartość $F_c$
----------------------|---------------
5 | `r round(qf(0.95,1, 5), 4)`
10 | `r round(qf(0.95,1, 10), 4)`
50 | `r round(qf(0.95,1, 50), 4)`

### Sprawdzenie, czy $t_c^2$ = $F_c$

Wartość r |Wartość $t_c^2$ | Wartość $F_c$
-----|-----------|--------------
5  | `r round(qt(0.975, 5)^2,4)` | `r round(qf(0.95,1, 5), 4)`
10 | `r round(qt(0.975, 10)^2,4)` | `r round(qf(0.95,1, 10), 4)`
50 | `r round(qt(0.975, 50)^2,4)` | `r round(qf(0.95,1, 50), 4)`

Widzimy, że $t_c^2$ = $F_c$.

## Zadanie 2

### Ile obserwacji znajduje się w pliku?

Skoro dfE = n-2 = 20, to wiemy, że n=22. Czyli mamy 22 obserwacje w pliku.

### Estymator $\sigma$.

Estymator $\sigma^2$, czyli $s^2$, możemy obliczyć ze wzoru:
$$s^2 = \frac{SSE}{dfE} = \frac{400}{20}=20$$

$$s = \sqrt{s^2} = \sqrt{20} = 2 \sqrt{5}$$
co wynosi w przybliżeniu `r round(sqrt(20),4)`.

### Sprawdź, czy slope jest równy 0.

$$H_0: b_1 = 0$$

Statystyka testowa F jest postaci $$F = \frac{MSM}{MSE}$$
$MSM = \frac{SSM}{dfM}=100$
$MSE = \frac{SSE}{dfE} = 20$

Zatem statystyka testowa F ma wartość $F = \frac{100}{20} = 5$ oraz liczbę stopni swobody 1 i 20.
Kwantyl $F_c$ dla $\alpha$=0.05 wynosi $F_c$ = `r round(qf(0.95, 1, 20), 4)`.
Widzimmy, że F>$F_c$, więc odrzucamy $H_0$.

### Jaką część zmienności zmiennej odpowiedzi wyjaśnia model?

W tym celu trzeba policzyć wartość współczynnika $R^2$, który wyraża się wzorem: $$R^2 = \frac{SSM}{SST}$$
gdzie SST = SSM + SSE = 500.
Podstawiając, otrzymujemy $R^2$ = 0.2.

### Próbkowy współczynnik korelacji między zmienną odpowiedzi a zmienną objaśniającą.

W tym podpunkcie interesuje nas wartość R. Znając $R^2$, możemy łatwo obliczyć R, który wynosi $`r round(sqrt(0.2), 4)`.